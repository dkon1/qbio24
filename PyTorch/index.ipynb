{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18779b6",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Constructing a minimal language model (Bigram/GPT)\"\n",
    "author: \"Tommy Walsh\"\n",
    "date: \"2024-07-02\"\n",
    "categories: [analysis,how_to]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-summary: \"Show the code\"\n",
    "    code-tools: true\n",
    "    code-overflow: wrap\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668e7e8",
   "metadata": {},
   "source": [
    "A minimal generatively pretrained transformer langauge model architecture. This notebook details constructing and training a transformer framework using the Tiny Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8acb4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a9ca3",
   "metadata": {},
   "source": [
    "## Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22cc51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # independent sequences to process in parallel\n",
    "block_size = 8 # maximum context length for predictions\n",
    "max_iters = 3000 # maximum number of training iterations\n",
    "eval_interval = 300 # step at which to evaluate performance during training\n",
    "learning_rate = 1e-2\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b683e5f",
   "metadata": {},
   "source": [
    "## Retrieving and examining the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce69fe",
   "metadata": {},
   "source": [
    "The code chunk below fetches the tinyshakespeare dataset that is used in this notebook and retrieves the unique characters in the dataset to construct the vocabulary for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0629cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#!brew install wget\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58a7c7",
   "metadata": {},
   "source": [
    "## Encoding the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e9992",
   "metadata": {},
   "source": [
    "The code chunk below defines mappings for each element of the vocabulary to an integer representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15f00eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers (character level encoding)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b02b6b",
   "metadata": {},
   "source": [
    "This code chunk uses the mappings defined above to numerically encode the tinyshakespeare dataset and convert its datatype to a torch.tensor for work with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef33effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e07613",
   "metadata": {},
   "source": [
    "## Preparing the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d20f9",
   "metadata": {},
   "source": [
    "This code chunk splits the tinyshakespeare data such that the first 90% will be used to train the model and the remainder will be used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08f874bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c0354",
   "metadata": {},
   "source": [
    "This code chunk defines a function to generate batches of input and target data for the model to train or be evaluated on. Print statements are included to demonstrate the target of the model for different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e5cce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 8])\n",
      "tensor([[ 6,  0, 21, 44,  1, 61, 43,  1],\n",
      "        [58, 52, 43, 57, 57,  2,  1, 57],\n",
      "        [ 1, 59, 52, 39, 41, 46, 47, 52],\n",
      "        [43, 42, 50, 39, 56,  6,  1, 50],\n",
      "        [59,  1, 57, 53,  1, 59, 54, 53],\n",
      "        [53, 44,  0, 46, 39, 52, 45, 47],\n",
      "        [59,  1, 52, 53, 58,  1, 42, 53],\n",
      "        [59, 45, 45, 43, 57, 58, 43, 42],\n",
      "        [46, 43, 52,  1, 41, 39, 51, 43],\n",
      "        [ 1, 41, 59, 56, 43,  1, 47, 58],\n",
      "        [59, 50, 50, 57, 11,  1, 39, 52],\n",
      "        [39, 64, 43,  1, 53, 44,  1, 56],\n",
      "        [59, 57, 58,  1, 49, 52, 53, 61],\n",
      "        [ 1, 46, 47, 57,  1, 54, 39, 57],\n",
      "        [ 1, 53, 58, 46, 43, 56,  1, 44],\n",
      "        [ 1, 50, 39, 61,  1, 46, 53, 50],\n",
      "        [42,  1, 45, 53, 53, 42, 52, 43],\n",
      "        [60, 47, 57, 46,  7, 51, 39, 56],\n",
      "        [58,  1, 61, 43, 56, 43,  1, 42],\n",
      "        [43,  1, 45, 56, 43, 39, 58,  1],\n",
      "        [39, 63, 57, 58,  1, 58, 46, 53],\n",
      "        [47, 51, 43, 52, 58,  1, 61, 47],\n",
      "        [47, 57, 50, 39, 52, 42,  1, 45],\n",
      "        [63, 43, 39,  6,  1, 51, 63,  1],\n",
      "        [46, 39, 60, 43,  0, 40, 43, 43],\n",
      "        [ 1, 58, 53,  1, 42, 59, 50, 50],\n",
      "        [41, 43,  6,  0, 19, 47, 60, 43],\n",
      "        [39,  1, 51, 53, 52, 58, 46,  0],\n",
      "        [ 0, 28, 43, 39, 41, 43,  6,  1],\n",
      "        [39, 58, 43,  1, 40, 56, 39, 47],\n",
      "        [57, 46, 39, 51, 43, 57, 58,  1],\n",
      "        [ 1, 40, 39, 41, 49,  1, 39, 52]], device='mps:0')\n",
      "targets:\n",
      "torch.Size([32, 8])\n",
      "tensor([[ 0, 21, 44,  1, 61, 43,  1, 61],\n",
      "        [52, 43, 57, 57,  2,  1, 57, 43],\n",
      "        [59, 52, 39, 41, 46, 47, 52, 45],\n",
      "        [42, 50, 39, 56,  6,  1, 50, 43],\n",
      "        [ 1, 57, 53,  1, 59, 54, 53, 52],\n",
      "        [44,  0, 46, 39, 52, 45, 47, 52],\n",
      "        [ 1, 52, 53, 58,  1, 42, 53,  1],\n",
      "        [45, 45, 43, 57, 58, 43, 42,  0],\n",
      "        [43, 52,  1, 41, 39, 51, 43,  1],\n",
      "        [41, 59, 56, 43,  1, 47, 58,  6],\n",
      "        [50, 50, 57, 11,  1, 39, 52, 42],\n",
      "        [64, 43,  1, 53, 44,  1, 56, 47],\n",
      "        [57, 58,  1, 49, 52, 53, 61,  1],\n",
      "        [46, 47, 57,  1, 54, 39, 57, 57],\n",
      "        [53, 58, 46, 43, 56,  1, 44, 47],\n",
      "        [50, 39, 61,  1, 46, 53, 50, 42],\n",
      "        [ 1, 45, 53, 53, 42, 52, 43, 57],\n",
      "        [47, 57, 46,  7, 51, 39, 56, 49],\n",
      "        [ 1, 61, 43, 56, 43,  1, 42, 47],\n",
      "        [ 1, 45, 56, 43, 39, 58,  1, 13],\n",
      "        [63, 57, 58,  1, 58, 46, 53, 59],\n",
      "        [51, 43, 52, 58,  1, 61, 47, 50],\n",
      "        [57, 50, 39, 52, 42,  1, 45, 47],\n",
      "        [43, 39,  6,  1, 51, 63,  1, 45],\n",
      "        [39, 60, 43,  0, 40, 43, 43, 52],\n",
      "        [58, 53,  1, 42, 59, 50, 50,  1],\n",
      "        [43,  6,  0, 19, 47, 60, 43,  1],\n",
      "        [ 1, 51, 53, 52, 58, 46,  0,  5],\n",
      "        [28, 43, 39, 41, 43,  6,  1, 46],\n",
      "        [58, 43,  1, 40, 56, 39, 47, 52],\n",
      "        [46, 39, 51, 43, 57, 58,  1, 58],\n",
      "        [40, 39, 41, 49,  1, 39, 52, 42]], device='mps:0')\n",
      "----\n",
      "when input is [6] the target: 0\n",
      "when input is [6, 0] the target: 21\n",
      "when input is [6, 0, 21] the target: 44\n",
      "when input is [6, 0, 21, 44] the target: 1\n",
      "when input is [6, 0, 21, 44, 1] the target: 61\n",
      "when input is [6, 0, 21, 44, 1, 61] the target: 43\n",
      "when input is [6, 0, 21, 44, 1, 61, 43] the target: 1\n",
      "when input is [6, 0, 21, 44, 1, 61, 43, 1] the target: 61\n",
      "when input is [58] the target: 52\n",
      "when input is [58, 52] the target: 43\n",
      "when input is [58, 52, 43] the target: 57\n",
      "when input is [58, 52, 43, 57] the target: 57\n",
      "when input is [58, 52, 43, 57, 57] the target: 2\n",
      "when input is [58, 52, 43, 57, 57, 2] the target: 1\n",
      "when input is [58, 52, 43, 57, 57, 2, 1] the target: 57\n",
      "when input is [58, 52, 43, 57, 57, 2, 1, 57] the target: 43\n",
      "when input is [1] the target: 59\n",
      "when input is [1, 59] the target: 52\n",
      "when input is [1, 59, 52] the target: 39\n",
      "when input is [1, 59, 52, 39] the target: 41\n",
      "when input is [1, 59, 52, 39, 41] the target: 46\n",
      "when input is [1, 59, 52, 39, 41, 46] the target: 47\n",
      "when input is [1, 59, 52, 39, 41, 46, 47] the target: 52\n",
      "when input is [1, 59, 52, 39, 41, 46, 47, 52] the target: 45\n",
      "when input is [43] the target: 42\n",
      "when input is [43, 42] the target: 50\n",
      "when input is [43, 42, 50] the target: 39\n",
      "when input is [43, 42, 50, 39] the target: 56\n",
      "when input is [43, 42, 50, 39, 56] the target: 6\n",
      "when input is [43, 42, 50, 39, 56, 6] the target: 1\n",
      "when input is [43, 42, 50, 39, 56, 6, 1] the target: 50\n",
      "when input is [43, 42, 50, 39, 56, 6, 1, 50] the target: 43\n",
      "when input is [59] the target: 1\n",
      "when input is [59, 1] the target: 57\n",
      "when input is [59, 1, 57] the target: 53\n",
      "when input is [59, 1, 57, 53] the target: 1\n",
      "when input is [59, 1, 57, 53, 1] the target: 59\n",
      "when input is [59, 1, 57, 53, 1, 59] the target: 54\n",
      "when input is [59, 1, 57, 53, 1, 59, 54] the target: 53\n",
      "when input is [59, 1, 57, 53, 1, 59, 54, 53] the target: 52\n",
      "when input is [53] the target: 44\n",
      "when input is [53, 44] the target: 0\n",
      "when input is [53, 44, 0] the target: 46\n",
      "when input is [53, 44, 0, 46] the target: 39\n",
      "when input is [53, 44, 0, 46, 39] the target: 52\n",
      "when input is [53, 44, 0, 46, 39, 52] the target: 45\n",
      "when input is [53, 44, 0, 46, 39, 52, 45] the target: 47\n",
      "when input is [53, 44, 0, 46, 39, 52, 45, 47] the target: 52\n",
      "when input is [59] the target: 1\n",
      "when input is [59, 1] the target: 52\n",
      "when input is [59, 1, 52] the target: 53\n",
      "when input is [59, 1, 52, 53] the target: 58\n",
      "when input is [59, 1, 52, 53, 58] the target: 1\n",
      "when input is [59, 1, 52, 53, 58, 1] the target: 42\n",
      "when input is [59, 1, 52, 53, 58, 1, 42] the target: 53\n",
      "when input is [59, 1, 52, 53, 58, 1, 42, 53] the target: 1\n",
      "when input is [59] the target: 45\n",
      "when input is [59, 45] the target: 45\n",
      "when input is [59, 45, 45] the target: 43\n",
      "when input is [59, 45, 45, 43] the target: 57\n",
      "when input is [59, 45, 45, 43, 57] the target: 58\n",
      "when input is [59, 45, 45, 43, 57, 58] the target: 43\n",
      "when input is [59, 45, 45, 43, 57, 58, 43] the target: 42\n",
      "when input is [59, 45, 45, 43, 57, 58, 43, 42] the target: 0\n",
      "when input is [46] the target: 43\n",
      "when input is [46, 43] the target: 52\n",
      "when input is [46, 43, 52] the target: 1\n",
      "when input is [46, 43, 52, 1] the target: 41\n",
      "when input is [46, 43, 52, 1, 41] the target: 39\n",
      "when input is [46, 43, 52, 1, 41, 39] the target: 51\n",
      "when input is [46, 43, 52, 1, 41, 39, 51] the target: 43\n",
      "when input is [46, 43, 52, 1, 41, 39, 51, 43] the target: 1\n",
      "when input is [1] the target: 41\n",
      "when input is [1, 41] the target: 59\n",
      "when input is [1, 41, 59] the target: 56\n",
      "when input is [1, 41, 59, 56] the target: 43\n",
      "when input is [1, 41, 59, 56, 43] the target: 1\n",
      "when input is [1, 41, 59, 56, 43, 1] the target: 47\n",
      "when input is [1, 41, 59, 56, 43, 1, 47] the target: 58\n",
      "when input is [1, 41, 59, 56, 43, 1, 47, 58] the target: 6\n",
      "when input is [59] the target: 50\n",
      "when input is [59, 50] the target: 50\n",
      "when input is [59, 50, 50] the target: 57\n",
      "when input is [59, 50, 50, 57] the target: 11\n",
      "when input is [59, 50, 50, 57, 11] the target: 1\n",
      "when input is [59, 50, 50, 57, 11, 1] the target: 39\n",
      "when input is [59, 50, 50, 57, 11, 1, 39] the target: 52\n",
      "when input is [59, 50, 50, 57, 11, 1, 39, 52] the target: 42\n",
      "when input is [39] the target: 64\n",
      "when input is [39, 64] the target: 43\n",
      "when input is [39, 64, 43] the target: 1\n",
      "when input is [39, 64, 43, 1] the target: 53\n",
      "when input is [39, 64, 43, 1, 53] the target: 44\n",
      "when input is [39, 64, 43, 1, 53, 44] the target: 1\n",
      "when input is [39, 64, 43, 1, 53, 44, 1] the target: 56\n",
      "when input is [39, 64, 43, 1, 53, 44, 1, 56] the target: 47\n",
      "when input is [59] the target: 57\n",
      "when input is [59, 57] the target: 58\n",
      "when input is [59, 57, 58] the target: 1\n",
      "when input is [59, 57, 58, 1] the target: 49\n",
      "when input is [59, 57, 58, 1, 49] the target: 52\n",
      "when input is [59, 57, 58, 1, 49, 52] the target: 53\n",
      "when input is [59, 57, 58, 1, 49, 52, 53] the target: 61\n",
      "when input is [59, 57, 58, 1, 49, 52, 53, 61] the target: 1\n",
      "when input is [1] the target: 46\n",
      "when input is [1, 46] the target: 47\n",
      "when input is [1, 46, 47] the target: 57\n",
      "when input is [1, 46, 47, 57] the target: 1\n",
      "when input is [1, 46, 47, 57, 1] the target: 54\n",
      "when input is [1, 46, 47, 57, 1, 54] the target: 39\n",
      "when input is [1, 46, 47, 57, 1, 54, 39] the target: 57\n",
      "when input is [1, 46, 47, 57, 1, 54, 39, 57] the target: 57\n",
      "when input is [1] the target: 53\n",
      "when input is [1, 53] the target: 58\n",
      "when input is [1, 53, 58] the target: 46\n",
      "when input is [1, 53, 58, 46] the target: 43\n",
      "when input is [1, 53, 58, 46, 43] the target: 56\n",
      "when input is [1, 53, 58, 46, 43, 56] the target: 1\n",
      "when input is [1, 53, 58, 46, 43, 56, 1] the target: 44\n",
      "when input is [1, 53, 58, 46, 43, 56, 1, 44] the target: 47\n",
      "when input is [1] the target: 50\n",
      "when input is [1, 50] the target: 39\n",
      "when input is [1, 50, 39] the target: 61\n",
      "when input is [1, 50, 39, 61] the target: 1\n",
      "when input is [1, 50, 39, 61, 1] the target: 46\n",
      "when input is [1, 50, 39, 61, 1, 46] the target: 53\n",
      "when input is [1, 50, 39, 61, 1, 46, 53] the target: 50\n",
      "when input is [1, 50, 39, 61, 1, 46, 53, 50] the target: 42\n",
      "when input is [42] the target: 1\n",
      "when input is [42, 1] the target: 45\n",
      "when input is [42, 1, 45] the target: 53\n",
      "when input is [42, 1, 45, 53] the target: 53\n",
      "when input is [42, 1, 45, 53, 53] the target: 42\n",
      "when input is [42, 1, 45, 53, 53, 42] the target: 52\n",
      "when input is [42, 1, 45, 53, 53, 42, 52] the target: 43\n",
      "when input is [42, 1, 45, 53, 53, 42, 52, 43] the target: 57\n",
      "when input is [60] the target: 47\n",
      "when input is [60, 47] the target: 57\n",
      "when input is [60, 47, 57] the target: 46\n",
      "when input is [60, 47, 57, 46] the target: 7\n",
      "when input is [60, 47, 57, 46, 7] the target: 51\n",
      "when input is [60, 47, 57, 46, 7, 51] the target: 39\n",
      "when input is [60, 47, 57, 46, 7, 51, 39] the target: 56\n",
      "when input is [60, 47, 57, 46, 7, 51, 39, 56] the target: 49\n",
      "when input is [58] the target: 1\n",
      "when input is [58, 1] the target: 61\n",
      "when input is [58, 1, 61] the target: 43\n",
      "when input is [58, 1, 61, 43] the target: 56\n",
      "when input is [58, 1, 61, 43, 56] the target: 43\n",
      "when input is [58, 1, 61, 43, 56, 43] the target: 1\n",
      "when input is [58, 1, 61, 43, 56, 43, 1] the target: 42\n",
      "when input is [58, 1, 61, 43, 56, 43, 1, 42] the target: 47\n",
      "when input is [43] the target: 1\n",
      "when input is [43, 1] the target: 45\n",
      "when input is [43, 1, 45] the target: 56\n",
      "when input is [43, 1, 45, 56] the target: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [43, 1, 45, 56, 43] the target: 39\n",
      "when input is [43, 1, 45, 56, 43, 39] the target: 58\n",
      "when input is [43, 1, 45, 56, 43, 39, 58] the target: 1\n",
      "when input is [43, 1, 45, 56, 43, 39, 58, 1] the target: 13\n",
      "when input is [39] the target: 63\n",
      "when input is [39, 63] the target: 57\n",
      "when input is [39, 63, 57] the target: 58\n",
      "when input is [39, 63, 57, 58] the target: 1\n",
      "when input is [39, 63, 57, 58, 1] the target: 58\n",
      "when input is [39, 63, 57, 58, 1, 58] the target: 46\n",
      "when input is [39, 63, 57, 58, 1, 58, 46] the target: 53\n",
      "when input is [39, 63, 57, 58, 1, 58, 46, 53] the target: 59\n",
      "when input is [47] the target: 51\n",
      "when input is [47, 51] the target: 43\n",
      "when input is [47, 51, 43] the target: 52\n",
      "when input is [47, 51, 43, 52] the target: 58\n",
      "when input is [47, 51, 43, 52, 58] the target: 1\n",
      "when input is [47, 51, 43, 52, 58, 1] the target: 61\n",
      "when input is [47, 51, 43, 52, 58, 1, 61] the target: 47\n",
      "when input is [47, 51, 43, 52, 58, 1, 61, 47] the target: 50\n",
      "when input is [47] the target: 57\n",
      "when input is [47, 57] the target: 50\n",
      "when input is [47, 57, 50] the target: 39\n",
      "when input is [47, 57, 50, 39] the target: 52\n",
      "when input is [47, 57, 50, 39, 52] the target: 42\n",
      "when input is [47, 57, 50, 39, 52, 42] the target: 1\n",
      "when input is [47, 57, 50, 39, 52, 42, 1] the target: 45\n",
      "when input is [47, 57, 50, 39, 52, 42, 1, 45] the target: 47\n",
      "when input is [63] the target: 43\n",
      "when input is [63, 43] the target: 39\n",
      "when input is [63, 43, 39] the target: 6\n",
      "when input is [63, 43, 39, 6] the target: 1\n",
      "when input is [63, 43, 39, 6, 1] the target: 51\n",
      "when input is [63, 43, 39, 6, 1, 51] the target: 63\n",
      "when input is [63, 43, 39, 6, 1, 51, 63] the target: 1\n",
      "when input is [63, 43, 39, 6, 1, 51, 63, 1] the target: 45\n",
      "when input is [46] the target: 39\n",
      "when input is [46, 39] the target: 60\n",
      "when input is [46, 39, 60] the target: 43\n",
      "when input is [46, 39, 60, 43] the target: 0\n",
      "when input is [46, 39, 60, 43, 0] the target: 40\n",
      "when input is [46, 39, 60, 43, 0, 40] the target: 43\n",
      "when input is [46, 39, 60, 43, 0, 40, 43] the target: 43\n",
      "when input is [46, 39, 60, 43, 0, 40, 43, 43] the target: 52\n",
      "when input is [1] the target: 58\n",
      "when input is [1, 58] the target: 53\n",
      "when input is [1, 58, 53] the target: 1\n",
      "when input is [1, 58, 53, 1] the target: 42\n",
      "when input is [1, 58, 53, 1, 42] the target: 59\n",
      "when input is [1, 58, 53, 1, 42, 59] the target: 50\n",
      "when input is [1, 58, 53, 1, 42, 59, 50] the target: 50\n",
      "when input is [1, 58, 53, 1, 42, 59, 50, 50] the target: 1\n",
      "when input is [41] the target: 43\n",
      "when input is [41, 43] the target: 6\n",
      "when input is [41, 43, 6] the target: 0\n",
      "when input is [41, 43, 6, 0] the target: 19\n",
      "when input is [41, 43, 6, 0, 19] the target: 47\n",
      "when input is [41, 43, 6, 0, 19, 47] the target: 60\n",
      "when input is [41, 43, 6, 0, 19, 47, 60] the target: 43\n",
      "when input is [41, 43, 6, 0, 19, 47, 60, 43] the target: 1\n",
      "when input is [39] the target: 1\n",
      "when input is [39, 1] the target: 51\n",
      "when input is [39, 1, 51] the target: 53\n",
      "when input is [39, 1, 51, 53] the target: 52\n",
      "when input is [39, 1, 51, 53, 52] the target: 58\n",
      "when input is [39, 1, 51, 53, 52, 58] the target: 46\n",
      "when input is [39, 1, 51, 53, 52, 58, 46] the target: 0\n",
      "when input is [39, 1, 51, 53, 52, 58, 46, 0] the target: 5\n",
      "when input is [0] the target: 28\n",
      "when input is [0, 28] the target: 43\n",
      "when input is [0, 28, 43] the target: 39\n",
      "when input is [0, 28, 43, 39] the target: 41\n",
      "when input is [0, 28, 43, 39, 41] the target: 43\n",
      "when input is [0, 28, 43, 39, 41, 43] the target: 6\n",
      "when input is [0, 28, 43, 39, 41, 43, 6] the target: 1\n",
      "when input is [0, 28, 43, 39, 41, 43, 6, 1] the target: 46\n",
      "when input is [39] the target: 58\n",
      "when input is [39, 58] the target: 43\n",
      "when input is [39, 58, 43] the target: 1\n",
      "when input is [39, 58, 43, 1] the target: 40\n",
      "when input is [39, 58, 43, 1, 40] the target: 56\n",
      "when input is [39, 58, 43, 1, 40, 56] the target: 39\n",
      "when input is [39, 58, 43, 1, 40, 56, 39] the target: 47\n",
      "when input is [39, 58, 43, 1, 40, 56, 39, 47] the target: 52\n",
      "when input is [57] the target: 46\n",
      "when input is [57, 46] the target: 39\n",
      "when input is [57, 46, 39] the target: 51\n",
      "when input is [57, 46, 39, 51] the target: 43\n",
      "when input is [57, 46, 39, 51, 43] the target: 57\n",
      "when input is [57, 46, 39, 51, 43, 57] the target: 58\n",
      "when input is [57, 46, 39, 51, 43, 57, 58] the target: 1\n",
      "when input is [57, 46, 39, 51, 43, 57, 58, 1] the target: 58\n",
      "when input is [1] the target: 40\n",
      "when input is [1, 40] the target: 39\n",
      "when input is [1, 40, 39] the target: 41\n",
      "when input is [1, 40, 39, 41] the target: 49\n",
      "when input is [1, 40, 39, 41, 49] the target: 1\n",
      "when input is [1, 40, 39, 41, 49, 1] the target: 39\n",
      "when input is [1, 40, 39, 41, 49, 1, 39] the target: 52\n",
      "when input is [1, 40, 39, 41, 49, 1, 39, 52] the target: 42\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # Create input batch x by stacking sequences of length 'block_size' starting at the indices 'ix'\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # Create target batch y by shifting x by one position to the right\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "# loop through batches to print target and context pairs\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4466ce",
   "metadata": {},
   "source": [
    "This code chunk defines a function to estimate the training and validation loss of the model without updating its parameters. The estimate_loss function temporarily sets the model to evaluation mode, computes the loss on several batches for both training and validation sets, and returns the average loss for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3676ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)  # Initialize a tensor to store loss values\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)  # Sample a batch of data\n",
    "            logits, loss = model(X, Y)  # Get logits and loss for the batch\n",
    "            losses[k] = loss.item()  # Store the loss value\n",
    "        out[split] = losses.mean()  # Compute the mean loss for the split\n",
    "    model.train()  # Set the model back to training mode\n",
    "    return out  # Return the average losses for both splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1592ef",
   "metadata": {},
   "source": [
    "## Construct a Bigram language model\n",
    "This code chunk constructs a Bigram language model. The model is designed to predict the next token in a sequence based on the current token. The main components and steps include:\n",
    " * Bigram Language Model Class: Defines the architecture of the model, including an embedding table and methods for forward propagation and token generation.\n",
    " * Forward Method: Computes the logits for the next token and calculates the loss if targets are provided.\n",
    " * Generate Method: Generates a sequence of tokens by predicting one token at a time and appending it to the current context.\n",
    " * Model Initialization and Testing: Initializes the model, computes logits and loss for a batch of data, and generates a sample sequence of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8de0eda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.6485, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "UNasE3QKdYMjKfxcq-PyQbRF.\n",
      "jxuUfZWievNL:C&v-jkcECOIiyeg zbZAcQ?yObr&MkzeAmyFXSPHd,j&?oneOAvrFotKuLTDx\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # use each tokens ebedding as the logits for the next token prediction\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size).to(device)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # calculate cross-entropy loss of the model\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        idx = idx.to(device)\n",
    "        if targets is not None:\n",
    "            targets = targets.to(device)\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshaping input to F.cross_entropy according to torch documentation\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        idx = idx.to(device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649f98f",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "This code chunk trains the Bigram language model using the AdamW optimizer. The training loop runs for a specified number of steps, during which it repeatedly samples batches of training data, evaluates the model's loss, and updates the model's parameters to minimize the loss. The goal is to optimize the model to predict the next token in a sequence based on the current context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93aa3d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6162, val loss 4.6193\n",
      "step 300: train loss 2.7867, val loss 2.8070\n",
      "step 600: train loss 2.5457, val loss 2.5681\n",
      "step 900: train loss 2.5047, val loss 2.5133\n",
      "step 1200: train loss 2.4916, val loss 2.5032\n",
      "step 1500: train loss 2.4728, val loss 2.4965\n",
      "step 1800: train loss 2.4674, val loss 2.4934\n",
      "step 2100: train loss 2.4683, val loss 2.4885\n",
      "step 2400: train loss 2.4610, val loss 2.4943\n",
      "step 2700: train loss 2.4542, val loss 2.4911\n",
      "\n",
      "HEayo is mpery way avend oubur'er sickes bokecard nhiceny\n",
      "\n",
      "He tw el fe oupise he, lbustselownthous;\n",
      "I m w\n",
      "T:\n",
      "TIONTouly me EUSerk mondrn itheland's oe, oghithet f, badogienthofathatey foueay wad,\n",
      "ureisold array n\n",
      "ICoyockield, wins, in mamybalorthyongmyooe, d Vofetthindy hak shil brveseay alsteanerm to, oupomp rede d pre h, gavitfithrer'GE apsts lathindKIO:\n",
      "Berouerse IOLUEDid nghathicerire.\n",
      "In IS:\n",
      "IOMISpequt f keithurin ne d An myorerrofe fisck.\n",
      "MUCI t wovyononoru he nd youlliler pt iciHATh y onee\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "loss_values = []\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f482984",
   "metadata": {},
   "source": [
    "Plot the training loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0dac4a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b13bcb",
   "metadata": {},
   "source": [
    "## Define attention blocks for a transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb11ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # independent sequences to process in parallel\n",
    "block_size = 64 # maximum context length for predictions\n",
    "max_iters = 50000 # total number of training iterations\n",
    "eval_interval = 500 # steps at which to evaluate on validation set\n",
    "learning_rate = 1e-3 # optimizer step\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "eval_iters = 200 #number of batches to estimate loss\n",
    "n_embd = 128 # dimensionality of token embeddings\n",
    "n_head = 2 # numby of heads in MHA\n",
    "n_layer = 2 # number of transformer layers\n",
    "dropout = 0.2 # dropout probability for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094f58c",
   "metadata": {},
   "source": [
    "This code chunk defines a single head of self-attention for a multi-head attention mechanism in a transformer model. The Head class implements the necessary operations to compute the self-attention scores and apply them to the input data, including the generation of key, query, and value projections, as well as handling the attention mask to prevent attending to future positions.\n",
    " * Initialization:\n",
    "     + The Head class inherits from nn.Module and represents one head of the self-attention mechanism.\n",
    "     + Linear layers (nn.Linear) are used to project the input into key, query, and value vectors of size head_size. The bias=False argument indicates that no bias term is added in these linear projections.\n",
    "     + A lower triangular matrix (tril) is registered as a buffer. This matrix is used to mask out future positions in the sequence to prevent attention from looking forward in time.\n",
    "     + A dropout layer (nn.Dropout) is added for regularization to prevent overfitting.\n",
    " * Forward Method:\n",
    "     + The forward method takes the input tensor x and computes the key (k), query (q), and value (v) projections.\n",
    "     + Attention scores are computed by taking the dot product of the query and the transpose of the key, scaled by the square root of the key's dimensionality to maintain a stable gradient.\n",
    "     + The attention scores are masked using the lower triangular matrix to ensure that each position can only attend to previous positions (and itself) but not future positions.\n",
    "     + The masked attention scores are normalized using the softmax function to convert them into probabilities.\n",
    "     + Dropout is applied to the attention weights to introduce regularization.\n",
    "     + The weighted sum of the value vectors is computed using the attention weights and returned as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "128a9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # Linear layers to project the input to key, query, and value vectors\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # Lower triangular matrix used to mask out future positions in the sequence\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, time_steps, channels)\n",
    "        # Output shape: (batch_size, time_steps, head_size)\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute key, query, and value projections\n",
    "        k = self.key(x)   # Shape: (batch_size, time_steps, head_size)\n",
    "        q = self.query(x) # Shape: (batch_size, time_steps, head_size)\n",
    "        \n",
    "        # Compute attention scores (affinities)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # Shape: (batch_size, time_steps, time_steps)\n",
    "        \n",
    "        # Apply the attention mask to prevent attending to future positions\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Shape: (batch_size, time_steps, time_steps)\n",
    "        \n",
    "        # Apply softmax to normalize the attention scores\n",
    "        wei = F.softmax(wei, dim=-1) # Shape: (batch_size, time_steps, time_steps)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Compute the weighted sum of the value vectors\n",
    "        v = self.value(x) # Shape: (batch_size, time_steps, head_size)\n",
    "        out = wei @ v # Shape: (batch_size, time_steps, time_steps) @ (batch_size, time_steps, head_size) -> (batch_size, time_steps, head_size)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af34cc3",
   "metadata": {},
   "source": [
    "This code chunk defines the MultiHeadAttention class, which implements multiple heads of self-attention in parallel. Each head is responsible for attending to different parts of the input sequence independently. The outputs of all the heads are concatenated and projected back to the original embedding size. This class also includes dropout for regularization.\n",
    " * Initialization:\n",
    "     + The MultiHeadAttention class inherits from nn.Module and represents multiple self-attention heads working in parallel.\n",
    "     + A list of Head instances is created, each representing a self-attention head. The nn.ModuleList is used to store the heads so they can be properly registered as sub-modules.\n",
    "     + A linear layer (self.proj) is defined to project the concatenated outputs of all heads back to the original embedding size (n_embd).\n",
    "     + A dropout layer (self.dropout) is added for regularization to prevent overfitting.\n",
    " * Forward Method:\n",
    "     + The forward method takes the input tensor x and applies each self-attention head to it.\n",
    "     + The outputs from all the heads are concatenated along the last dimension (dim=-1), resulting in a tensor with dimensions (batch_size, time_steps, head_size * num_heads).\n",
    "     + The concatenated tensor is then projected back to the original embedding size using the linear layer, and dropout is applied for regularization.\n",
    "     + The final output tensor, with dimensions (batch_size, time_steps, n_embd), is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ada6d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Create a list of 'Head' instances, each representing a self-attention head\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Linear layer to project the concatenated output of all heads back to the embedding size\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each head to the input and concatenate the outputs along the last dimension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # Project the concatenated outputs back to the original embedding size and apply dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723172b9",
   "metadata": {},
   "source": [
    "This code chunk defines the FeedForward class, which implements a simple feedforward neural network layer used in transformer models. The layer consists of two linear transformations with a ReLU activation function in between, followed by a dropout layer for regularization.\n",
    " * Initialization:\n",
    "     + The FeedForward class inherits from nn.Module and represents a feedforward neural network layer.\n",
    "     + The nn.Sequential module is used to define a sequence of layers that will be applied to the input tensor x.\n",
    " * Layer Definitions:\n",
    "     + nn.Linear(n_embd, 4 * n_embd): A linear layer that projects the input from its original embedding size (n_embd) to a larger size (4 * n_embd). This expansion allows the model to learn more complex representations.\n",
    "     + nn.ReLU(): A ReLU (Rectified Linear Unit) activation function introduces non-linearity into the model, enabling it to learn more complex patterns.\n",
    "     + nn.Linear(4 * n_embd, n_embd): A second linear layer projects the expanded representation back to the original embedding size (n_embd).\n",
    "     + nn.Dropout(dropout): A dropout layer is added for regularization to prevent overfitting by randomly setting some of the input elements to zero during training.\n",
    " * Forward Method:\n",
    "     + The forward method takes the input tensor x and passes it through the defined sequence of layers.\n",
    "     + The final output tensor, after being processed by the feedforward network, is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8395d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        # Define the feedforward network as a sequence of layers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # First linear layer projects the input from n_embd to 4 * n_embd\n",
    "            nn.ReLU(),                      # ReLU activation function introduces non-linearity\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Second linear layer projects back to n_embd\n",
    "            nn.Dropout(dropout),            # Dropout layer for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the feedforward network\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90eafc",
   "metadata": {},
   "source": [
    "This code chunk defines the Block class, which represents a single transformer block. Each block consists of multi-head self-attention followed by a feedforward neural network, with layer normalization applied before each sub-layer. Residual connections are used to help with the training of deep networks.\n",
    " * Initialization:\n",
    "     + The Block class inherits from nn.Module and represents a single transformer block.\n",
    "     + head_size is calculated as the embedding dimension (n_embd) divided by the number of attention heads (n_head).\n",
    "     + self.sa initializes the multi-head attention mechanism using the MultiHeadAttention class.\n",
    "     + self.ffwd initializes the feedforward neural network using the FeedForward class.\n",
    "     + self.ln1 and self.ln2 are layer normalization layers that normalize the input before passing it to the multi-head attention and feedforward layers, respectively.\n",
    " * Forward Method:\n",
    "     + The forward method takes the input tensor x and applies the following steps:\n",
    "         - Layer Normalization and Multi-Head Attention: The input is first normalized using self.ln1(x), then passed through the multi-head attention mechanism (self.sa). The result is added to the original input (x) to form a residual connection.\n",
    "         - Layer Normalization and Feedforward Network: The output of the previous step is normalized using self.ln2(x), then passed through the feedforward network (self.ffwd). The result is added to the input of this step to form another residual connection.\n",
    "     + The final output tensor, after being processed by the transformer block, is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f2ac9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        # Calculate the size of each head\n",
    "        head_size = n_embd // n_head\n",
    "        # Multi-head attention mechanism\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        # Feedforward neural network\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        # Layer normalization applied before the attention and feedforward layers\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization followed by multi-head attention, then add the residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # Apply layer normalization followed by feedforward network, then add the residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e752177",
   "metadata": {},
   "source": [
    "## Define a GPT\n",
    "This code chunk defines the GPTLanguageModel class, which represents a transformer-based language model. The model includes token and position embeddings, multiple transformer blocks, and an output layer to generate logits for the next token predictions. It also includes methods for weight initialization, forward propagation, and token generation.\n",
    "\n",
    " * Initialization:\n",
    "     + self.token_embedding_table: Embedding layer that maps each token in the vocabulary to a dense vector of size n_embd.\n",
    "     + self.position_embedding_table: Embedding layer that maps each position in the sequence to a dense vector of size n_embd.\n",
    "     + self.blocks: A sequential container of Block instances, each representing a transformer block. The number of blocks is determined by n_layer.\n",
    "     + self.ln_f: Final layer normalization to ensure the output is normalized before the final linear projection.\n",
    "     + self.lm_head: Linear layer that projects the output of the transformer blocks to the vocabulary size, producing logits for each token.\n",
    "     + Custom weight initialization is applied to ensure better training convergence.\n",
    " * Forward Method:\n",
    "     + Computes token and position embeddings and sums them.\n",
    "     + Passes the resulting tensor through the transformer blocks.\n",
    "     + Applies layer normalization to the output of the transformer blocks.\n",
    "     + Projects the output to logits using the linear layer.\n",
    "     + Computes the cross-entropy loss if targets are provided. This is used during training to measure the prediction error.\n",
    " * Generate Method:\n",
    "     + Generates new tokens based on the current context.\n",
    "     + Iteratively appends new tokens to the context until the specified number of new tokens is generated.\n",
    "     + Crops the input sequence to the last block_size tokens to ensure the model operates within its context window.\n",
    "     + Computes logits and applies softmax to get probabilities for the next token.\n",
    "     + Samples the next token from the probability distribution and appends it to the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e459c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Token embedding table: maps each token to a dense vector\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # Position embedding table: maps each position to a dense vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # Sequential container of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        # Linear layer to project transformer outputs to vocabulary size logits\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # Apply custom weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # Custom weight initialization\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Batch size (B) and time steps (T)\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Get token embeddings\n",
    "        tok_emb = self.token_embedding_table(idx).to(device) # (B, T, C)\n",
    "        # Get position embeddings\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # Sum token and position embeddings\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        # Pass through transformer blocks\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        # Compute logits\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T).to(device)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Generate new tokens given the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:].to(device)\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f03d9a",
   "metadata": {},
   "source": [
    "## Train the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7bce7a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.420929 M parameters\n",
      "step 0: train loss 4.2461, val loss 4.2464\n",
      "step 500: train loss 2.0895, val loss 2.1367\n",
      "step 1000: train loss 1.9291, val loss 2.0205\n",
      "step 1500: train loss 1.8345, val loss 1.9718\n",
      "step 2000: train loss 1.7796, val loss 1.9283\n",
      "step 2500: train loss 1.7526, val loss 1.9263\n",
      "step 3000: train loss 1.7235, val loss 1.8815\n",
      "step 3500: train loss 1.7092, val loss 1.8826\n",
      "step 4000: train loss 1.6787, val loss 1.8450\n",
      "step 4500: train loss 1.6663, val loss 1.8363\n",
      "step 5000: train loss 1.6570, val loss 1.8231\n",
      "step 5500: train loss 1.6296, val loss 1.8187\n",
      "step 6000: train loss 1.6309, val loss 1.8086\n",
      "step 6500: train loss 1.6183, val loss 1.7991\n",
      "step 7000: train loss 1.6036, val loss 1.7933\n",
      "step 7500: train loss 1.6086, val loss 1.7984\n",
      "step 8000: train loss 1.5887, val loss 1.7953\n",
      "step 8500: train loss 1.5810, val loss 1.7810\n",
      "step 9000: train loss 1.5898, val loss 1.7769\n",
      "step 9500: train loss 1.5695, val loss 1.7480\n",
      "step 10000: train loss 1.5683, val loss 1.7474\n",
      "step 10500: train loss 1.5582, val loss 1.7495\n",
      "step 11000: train loss 1.5566, val loss 1.7352\n",
      "step 11500: train loss 1.5485, val loss 1.7437\n",
      "step 12000: train loss 1.5515, val loss 1.7563\n",
      "step 12500: train loss 1.5408, val loss 1.7362\n",
      "step 13000: train loss 1.5362, val loss 1.7371\n",
      "step 13500: train loss 1.5366, val loss 1.7421\n",
      "step 14000: train loss 1.5306, val loss 1.7548\n",
      "step 14500: train loss 1.5289, val loss 1.7224\n",
      "step 15000: train loss 1.5372, val loss 1.7273\n",
      "step 15500: train loss 1.5253, val loss 1.7156\n",
      "step 16000: train loss 1.5141, val loss 1.7238\n",
      "step 16500: train loss 1.5243, val loss 1.7167\n",
      "step 17000: train loss 1.5064, val loss 1.7138\n",
      "step 17500: train loss 1.5112, val loss 1.7034\n",
      "step 18000: train loss 1.5105, val loss 1.7147\n",
      "step 18500: train loss 1.5041, val loss 1.7033\n",
      "step 19000: train loss 1.4947, val loss 1.6941\n",
      "step 19500: train loss 1.5017, val loss 1.6914\n",
      "step 20000: train loss 1.5027, val loss 1.7165\n",
      "step 20500: train loss 1.4943, val loss 1.7093\n",
      "step 21000: train loss 1.4939, val loss 1.6938\n",
      "step 21500: train loss 1.4910, val loss 1.6932\n",
      "step 22000: train loss 1.4812, val loss 1.6998\n",
      "step 22500: train loss 1.4868, val loss 1.6952\n",
      "step 23000: train loss 1.4867, val loss 1.6868\n",
      "step 23500: train loss 1.4814, val loss 1.6916\n",
      "step 24000: train loss 1.4771, val loss 1.6921\n",
      "step 24500: train loss 1.4714, val loss 1.6726\n",
      "step 25000: train loss 1.4752, val loss 1.6790\n",
      "step 25500: train loss 1.4771, val loss 1.6865\n",
      "step 26000: train loss 1.4745, val loss 1.6834\n",
      "step 26500: train loss 1.4629, val loss 1.6825\n",
      "step 27000: train loss 1.4731, val loss 1.6716\n",
      "step 27500: train loss 1.4667, val loss 1.6696\n",
      "step 28000: train loss 1.4658, val loss 1.6715\n",
      "step 28500: train loss 1.4703, val loss 1.6646\n",
      "step 29000: train loss 1.4601, val loss 1.6699\n",
      "step 29500: train loss 1.4616, val loss 1.6589\n",
      "step 30000: train loss 1.4644, val loss 1.6684\n",
      "step 30500: train loss 1.4711, val loss 1.6739\n",
      "step 31000: train loss 1.4640, val loss 1.6713\n",
      "step 31500: train loss 1.4595, val loss 1.6498\n",
      "step 32000: train loss 1.4534, val loss 1.6703\n",
      "step 32500: train loss 1.4720, val loss 1.6811\n",
      "step 33000: train loss 1.4564, val loss 1.6665\n",
      "step 33500: train loss 1.4524, val loss 1.6521\n",
      "step 34000: train loss 1.4545, val loss 1.6593\n",
      "step 34500: train loss 1.4546, val loss 1.6596\n",
      "step 35000: train loss 1.4513, val loss 1.6799\n",
      "step 35500: train loss 1.4534, val loss 1.6711\n",
      "step 36000: train loss 1.4461, val loss 1.6675\n",
      "step 36500: train loss 1.4457, val loss 1.6583\n",
      "step 37000: train loss 1.4462, val loss 1.6524\n",
      "step 37500: train loss 1.4430, val loss 1.6479\n",
      "step 38000: train loss 1.4499, val loss 1.6531\n",
      "step 38500: train loss 1.4377, val loss 1.6576\n",
      "step 39000: train loss 1.4385, val loss 1.6560\n",
      "step 39500: train loss 1.4403, val loss 1.6446\n",
      "step 40000: train loss 1.4459, val loss 1.6579\n",
      "step 40500: train loss 1.4392, val loss 1.6565\n",
      "step 41000: train loss 1.4455, val loss 1.6486\n",
      "step 41500: train loss 1.4371, val loss 1.6613\n",
      "step 42000: train loss 1.4424, val loss 1.6409\n",
      "step 42500: train loss 1.4367, val loss 1.6545\n",
      "step 43000: train loss 1.4357, val loss 1.6430\n",
      "step 43500: train loss 1.4367, val loss 1.6449\n",
      "step 44000: train loss 1.4373, val loss 1.6621\n",
      "step 44500: train loss 1.4336, val loss 1.6438\n",
      "step 45000: train loss 1.4271, val loss 1.6641\n",
      "step 45500: train loss 1.4348, val loss 1.6430\n",
      "step 46000: train loss 1.4259, val loss 1.6544\n",
      "step 46500: train loss 1.4308, val loss 1.6569\n",
      "step 47000: train loss 1.4321, val loss 1.6447\n",
      "step 47500: train loss 1.4347, val loss 1.6489\n",
      "step 48000: train loss 1.4311, val loss 1.6288\n",
      "step 48500: train loss 1.4241, val loss 1.6386\n",
      "step 49000: train loss 1.4275, val loss 1.6467\n",
      "step 49500: train loss 1.4307, val loss 1.6496\n",
      "step 49999: train loss 1.4249, val loss 1.6592\n",
      "\n",
      "\n",
      "JOHN OF YORK:\n",
      "She woft,\n",
      "The groanting te gext cursh ensenge\n",
      "Which not Sirrel was that inquteo'\n",
      "Then: all men with thy prison that lillaces we go me great the in himarron,\n",
      "Onemy, the\n",
      "noble bring to--\n",
      "\n",
      "CROLANUS:\n",
      "Heard him towing from tany of ske.\n",
      "\n",
      "SICINIUS:\n",
      "Provost vengemantor. Therefore more not look.\n",
      "\n",
      "GROMEO:\n",
      "Ocefformentnes\n",
      "Er Englances.\n",
      "Ad me few our pocer, in thy enter shall in hour caltareness up of that your husband.\n",
      "It is bithee him asid but here, wishment questrents for live,\n",
      "The glace o'\n"
     ]
    }
   ],
   "source": [
    "# Initialize the GPT language model\n",
    "model = GPTLanguageModel()\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "m = model.to(device)\n",
    "# Print the number of parameters in the model (in millions)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# Create a PyTorch optimizer with AdamW\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "    # Every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of training data\n",
    "    xb, yb = get_batch('train')\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    # Evaluate the model to get logits and calculate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # Zero the gradients to prevent accumulation\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Backpropagate the loss to compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters using the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "# Generate text from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_text)\n",
    "\n",
    "# Uncomment the line below to generate a longer sequence and save to 'more.txt'\n",
    "# open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
